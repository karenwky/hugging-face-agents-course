{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721a5ae6",
   "metadata": {},
   "source": [
    "This is a follow-along notebook of *Introduction to the LlamaHub* from [Hugging Face Agents Course](https://huggingface.co/learn/agents-course/unit2/llama-index/llama-hub), with additional trials. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f17424d",
   "metadata": {},
   "source": [
    "# Introduction to the LlamaHub\n",
    "LlamaHub is a registry of hundreds of integrations, agents and tools that you can use within LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38baf80",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "LlamaIndex installation instructions are available as a well-structured overview on [LlamaHub](https://llamahub.ai/). Most of the installation commands generally follow an easy-to-remember format:\n",
    "```bash\n",
    "pip install llama-index-{component-type}-{framework-name}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded2e4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-huggingface-api\n",
      "  Downloading llama_index_llms_huggingface_api-0.4.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting llama-index-embeddings-huggingface\n",
      "  Downloading llama_index_embeddings_huggingface-0.5.4-py3-none-any.whl.metadata (458 bytes)\n",
      "Requirement already satisfied: huggingface-hub>=0.31.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-llms-huggingface-api) (0.31.1)\n",
      "Collecting llama-index-core<0.13,>=0.12.0 (from llama-index-llms-huggingface-api)\n",
      "  Downloading llama_index_core-0.12.36-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (3.11.18)\n",
      "Requirement already satisfied: aiosqlite in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (0.21.0)\n",
      "Collecting banks<3,>=2.0.0 (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading banks-2.1.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: dataclasses-json in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (1.2.18)\n",
      "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (2025.3.2)\n",
      "Requirement already satisfied: httpx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /Users/karen/Library/Python/3.12/lib/python/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (3.4.2)\n",
      "Collecting nltk>3.8.1 (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (1.26.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (10.3.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (2.11.4)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (2.32.3)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (2.0.40)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (1.20.0)\n",
      "Collecting griffe (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (3.1.4)\n",
      "Requirement already satisfied: platformdirs in /Users/karen/Library/Python/3.12/lib/python/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (4.3.7)\n",
      "Requirement already satisfied: idna>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (3.10)\n",
      "Collecting sentence-transformers>=2.6.1 (from llama-index-embeddings-huggingface)\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/karen/Library/Python/3.12/lib/python/site-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (24.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (1.1.0)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (2023.11.17)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.6.1)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.15.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.21.1)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: greenlet>=1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (3.2.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (69.0.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (3.26.1)\n",
      "Collecting colorama>=0.4 (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (0.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from anyio->httpx->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (2.1.5)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.6.0)\n",
      "Downloading llama_index_llms_huggingface_api-0.4.3-py3-none-any.whl (7.5 kB)\n",
      "Downloading llama_index_core-0.12.36-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading banks-2.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading llama_index_embeddings_huggingface-0.5.4-py3-none-any.whl (8.9 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: filetype, dirtyjson, safetensors, nltk, colorama, griffe, banks, transformers, llama-index-core, sentence-transformers, llama-index-llms-huggingface-api, llama-index-embeddings-huggingface\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [llama-index-embeddings-huggingface]ansformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed banks-2.1.2 colorama-0.4.6 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.7.3 llama-index-core-0.12.36 llama-index-embeddings-huggingface-0.5.4 llama-index-llms-huggingface-api-0.4.3 nltk-3.9.1 safetensors-0.5.3 sentence-transformers-4.1.0 transformers-4.51.3\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-llms-huggingface-api llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ddab687",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c17434f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"deepseek-r1:8b\", request_timeout=60.0)\n",
    "response = llm.complete(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7d9c438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='<think>\\n\\n</think>\\n\\nThe capital of France is Paris.', additional_kwargs={'tool_calls': []}, raw={'model': 'deepseek-r1:8b', 'created_at': '2025-05-15T15:37:01.200713Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10475338166, 'load_duration': 4068853125, 'prompt_eval_count': 10, 'prompt_eval_duration': 5952186625, 'eval_count': 12, 'eval_duration': 449796208, 'message': Message(role='assistant', content='<think>\\n\\n</think>\\n\\nThe capital of France is Paris.', images=None, tool_calls=None), 'usage': {'prompt_tokens': 10, 'completion_tokens': 12, 'total_tokens': 22}}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d054758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "011ab2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking for the capital of France. I need to make sure I provide the correct answer. Let me think... France is a country in Europe, and its capital is Paris. But wait, maybe the user is testing if I know any other possible capitals? No, I'm pretty sure Paris is the official capital. I should confirm that there's no other city that's considered the capital. Let me recall: France has several major cities like Lyon, Marseille, and Bordeaux, but none of them are the capital. The seat of government is definitely in Paris. So the answer should be Paris. I should also mention that it's the largest city in France and a major cultural center. That way, the user gets a bit more context beyond just the name. Alright, I think that's all.\n",
      "</think>\n",
      "\n",
      "The capital of France is **Paris**. It is also the largest city in France and a major cultural, economic, and political hub in Europe. 🇫🇷\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"qwen3:8b\", request_timeout=60.0)\n",
    "response = llm.complete(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63524a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text=\"<think>\\nOkay, the user is asking for the capital of France. I need to make sure I provide the correct answer. Let me think... France is a country in Europe, and its capital is Paris. But wait, maybe the user is testing if I know any other possible capitals? No, I'm pretty sure Paris is the official capital. I should confirm that there's no other city that's considered the capital. Let me recall: France has several major cities like Lyon, Marseille, and Bordeaux, but none of them are the capital. The seat of government is definitely in Paris. So the answer should be Paris. I should also mention that it's the largest city in France and a major cultural center. That way, the user gets a bit more context beyond just the name. Alright, I think that's all.\\n</think>\\n\\nThe capital of France is **Paris**. It is also the largest city in France and a major cultural, economic, and political hub in Europe. 🇫🇷\", additional_kwargs={'tool_calls': []}, raw={'model': 'qwen3:8b', 'created_at': '2025-05-15T15:37:55.852445Z', 'done': True, 'done_reason': 'stop', 'total_duration': 16183577542, 'load_duration': 4842445000, 'prompt_eval_count': 15, 'prompt_eval_duration': 2626562875, 'eval_count': 204, 'eval_duration': 8711659625, 'message': Message(role='assistant', content=\"<think>\\nOkay, the user is asking for the capital of France. I need to make sure I provide the correct answer. Let me think... France is a country in Europe, and its capital is Paris. But wait, maybe the user is testing if I know any other possible capitals? No, I'm pretty sure Paris is the official capital. I should confirm that there's no other city that's considered the capital. Let me recall: France has several major cities like Lyon, Marseille, and Bordeaux, but none of them are the capital. The seat of government is definitely in Paris. So the answer should be Paris. I should also mention that it's the largest city in France and a major cultural center. That way, the user gets a bit more context beyond just the name. Alright, I think that's all.\\n</think>\\n\\nThe capital of France is **Paris**. It is also the largest city in France and a major cultural, economic, and political hub in Europe. 🇫🇷\", images=None, tool_calls=None), 'usage': {'prompt_tokens': 15, 'completion_tokens': 204, 'total_tokens': 219}}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21832615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**. \n",
      "\n",
      "It’s a global center for art, fashion, gastronomy, and culture. 😊 \n",
      "\n",
      "Do you want to know anything more about Paris?\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"gemma3:4b\", request_timeout=60.0)\n",
    "response = llm.complete(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1320b0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='The capital of France is **Paris**. \\n\\nIt’s a global center for art, fashion, gastronomy, and culture. 😊 \\n\\nDo you want to know anything more about Paris?', additional_kwargs={'tool_calls': []}, raw={'model': 'gemma3:4b', 'created_at': '2025-05-15T15:38:40.805314Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4412092541, 'load_duration': 2574530791, 'prompt_eval_count': 16, 'prompt_eval_duration': 837850042, 'eval_count': 40, 'eval_duration': 998860417, 'message': Message(role='assistant', content='The capital of France is **Paris**. \\n\\nIt’s a global center for art, fashion, gastronomy, and culture. 😊 \\n\\nDo you want to know anything more about Paris?', images=None, tool_calls=None), 'usage': {'prompt_tokens': 16, 'completion_tokens': 40, 'total_tokens': 56}}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52b3d0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(\n",
    "    model=\"deepseek-r1:8b\", \n",
    "    request_timeout=60.0, \n",
    "    temperature=1)\n",
    "response = llm.complete(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7486f771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='<think>\\n\\n</think>\\n\\nThe capital of France is Paris.', additional_kwargs={'tool_calls': []}, raw={'model': 'deepseek-r1:8b', 'created_at': '2025-05-15T15:41:22.613773Z', 'done': True, 'done_reason': 'stop', 'total_duration': 9834955084, 'load_duration': 3795634000, 'prompt_eval_count': 10, 'prompt_eval_duration': 5600140250, 'eval_count': 12, 'eval_duration': 436595209, 'message': Message(role='assistant', content='<think>\\n\\n</think>\\n\\nThe capital of France is Paris.', images=None, tool_calls=None), 'usage': {'prompt_tokens': 10, 'completion_tokens': 12, 'total_tokens': 22}}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f45118e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the user is asking, \"What is the capital of France?\" Hmm, that's a pretty straightforward question. Let me think. I know that France is a country in Europe, and it's one of the most well-known countries there. The capital... I believe it's Paris. But wait, I should make sure I'm not mixing it up with another city. Sometimes people confuse Paris with other major cities in France, like Lyon or Marseille. But no, Lyon is the second-largest city, and Marseille is the largest port city. Paris is definitely the capital.\n",
      "\n",
      "Wait, is there any chance that the capital has changed? I don't think so. France has had Paris as its capital since the 3rd century, I believe. The city has been the political and cultural center for a long time. Even during the French Revolution, Paris was the capital. So, unless there's some recent change that I'm not aware of, which I don't think there is, the answer should be Paris. \n",
      "\n",
      "Let me also consider if there's any possibility of a trick question here. Maybe the user is testing my knowledge, but I don't see any reason to doubt. The question is pretty direct. So, to answer correctly, I just need to state that the capital of France is Paris. That's the standard answer. I don't think there's any need to go into more details unless the user asks for more information, like the population, history, or other facts about Paris. But since the question is simple, the answer is straightforward.\n",
      "</think>\n",
      "\n",
      "The capital of France is **Paris**. It is the political, economic, and cultural center of the country and is renowned for its historical landmarks, art, and fashion.\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(\n",
    "    model=\"qwen3:8b\", \n",
    "    request_timeout=60.0, \n",
    "    temperature=1)\n",
    "response = llm.complete(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1e9dc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "It seems like your message might be a typo or in a different language. Could you clarify or provide more context so I can assist you better? 😊\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(\n",
    "    model=\"deepseek-r1:8b\", \n",
    "    temperature=1)\n",
    "response = llm.complete(\".rewsna eht sa \\\"tfel\\\" drow eht fo etisoppo eht etirw ,ecnetnes siht dnatsrednu uoy fI\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0b61523",
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadTimeout",
     "evalue": "timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:216\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    215\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:196\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    195\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    200\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    201\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    202\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    203\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpcore/_sync/connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpcore/_sync/http11.py:143\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    142\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpcore/_sync/http11.py:113\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m    105\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m    106\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    107\u001b[39m     (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m     trace.return_value = (\n\u001b[32m    115\u001b[39m         http_version,\n\u001b[32m    116\u001b[39m         status,\n\u001b[32m    117\u001b[39m         reason_phrase,\n\u001b[32m    118\u001b[39m         headers,\n\u001b[32m    119\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpcore/_sync/http11.py:186\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpcore/_sync/http11.py:224\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    230\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    234\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpcore/_backends/sync.py:124\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    123\u001b[39m exc_map: ExceptionMapping = {socket.timeout: ReadTimeout, \u001b[38;5;167;01mOSError\u001b[39;00m: ReadError}\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43msettimeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m llm = Ollama(\n\u001b[32m      2\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mqwen3:8b\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m      3\u001b[39m     temperature=\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.rewsna eht sa \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[33;43mtfel\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[33;43m drow eht fo etisoppo eht etirw ,ecnetnes siht dnatsrednu uoy fI\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:324\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    326\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    327\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/llama_index/core/llms/callbacks.py:431\u001b[39m, in \u001b[36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[39m\u001b[34m(_self, *args, **kwargs)\u001b[39m\n\u001b[32m    422\u001b[39m event_id = callback_manager.on_event_start(\n\u001b[32m    423\u001b[39m     CBEventType.LLM,\n\u001b[32m    424\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m    428\u001b[39m     },\n\u001b[32m    429\u001b[39m )\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m     f_return_val = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    433\u001b[39m     callback_manager.on_event_end(\n\u001b[32m    434\u001b[39m         CBEventType.LLM,\n\u001b[32m    435\u001b[39m         payload={EventPayload.EXCEPTION: e},\n\u001b[32m    436\u001b[39m         event_id=event_id,\n\u001b[32m    437\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/llama_index/llms/ollama/base.py:511\u001b[39m, in \u001b[36mOllama.complete\u001b[39m\u001b[34m(self, prompt, formatted, **kwargs)\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;129m@llm_completion_callback\u001b[39m()\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcomplete\u001b[39m(\n\u001b[32m    509\u001b[39m     \u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, formatted: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs: Any\n\u001b[32m    510\u001b[39m ) -> CompletionResponse:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchat_to_completion_decorator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/llama_index/core/base/llms/generic_utils.py:179\u001b[39m, in \u001b[36mchat_to_completion_decorator.<locals>.wrapper\u001b[39m\u001b[34m(prompt, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m, **kwargs: Any) -> CompletionResponse:\n\u001b[32m    177\u001b[39m     \u001b[38;5;66;03m# normalize input\u001b[39;00m\n\u001b[32m    178\u001b[39m     messages = prompt_to_messages(prompt)\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     chat_response = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;66;03m# normalize output\u001b[39;00m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chat_response_to_completion_response(chat_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:324\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    326\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    327\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/llama_index/core/llms/callbacks.py:173\u001b[39m, in \u001b[36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[39m\u001b[34m(_self, messages, **kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m event_id = callback_manager.on_event_start(\n\u001b[32m    165\u001b[39m     CBEventType.LLM,\n\u001b[32m    166\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m    170\u001b[39m     },\n\u001b[32m    171\u001b[39m )\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     f_return_val = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    175\u001b[39m     callback_manager.on_event_end(\n\u001b[32m    176\u001b[39m         CBEventType.LLM,\n\u001b[32m    177\u001b[39m         payload={EventPayload.EXCEPTION: e},\n\u001b[32m    178\u001b[39m         event_id=event_id,\n\u001b[32m    179\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/llama_index/llms/ollama/base.py:322\u001b[39m, in \u001b[36mOllama.chat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m tools = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    320\u001b[39m \u001b[38;5;28mformat\u001b[39m = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.json_mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mollama_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m response = \u001b[38;5;28mdict\u001b[39m(response)\n\u001b[32m    334\u001b[39m tool_calls = response[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mtool_calls\u001b[39m\u001b[33m\"\u001b[39m, [])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ollama/_client.py:333\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    290\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    291\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    298\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    299\u001b[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[32m    300\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    302\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    331\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ollama/_client.py:178\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ollama/_client.py:118\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    117\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m     r.raise_for_status()\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpx/_transports/default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpcore\u001b[39;00m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    156\u001b[39m     value = typ()\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: timed out"
     ]
    }
   ],
   "source": [
    "llm = Ollama(\n",
    "    model=\"qwen3:8b\", \n",
    "    temperature=1)\n",
    "response = llm.complete(\".rewsna eht sa \\\"tfel\\\" drow eht fo etisoppo eht etirw ,ecnetnes siht dnatsrednu uoy fI\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9809be68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
