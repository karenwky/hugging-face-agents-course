{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b17305e",
   "metadata": {},
   "source": [
    "This is a follow-along notebook of [Tools in LlamaIndex](https://colab.research.google.com/#fileId=https%3A//huggingface.co/agents-course/notebooks/blob/main/unit2/llama-index/tools.ipynb) from [Hugging Face Agents Course](https://huggingface.co/learn/agents-course/unit2/llama-index/tools), with additional trials. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9649ee6d",
   "metadata": {},
   "source": [
    "# Tools in LlamaIndex\n",
    "There areÂ four main types of tools in LlamaIndex:\n",
    "1.  `FunctionTool`: Convert any Python function into a tool that an agent can use. It automatically figures out how the function works.\n",
    "2.  `QueryEngineTool`: A tool that lets agents use query engines. Since agents are built on query engines, they can also use other agents as tools.\n",
    "3.  `Toolspecs`: Sets of tools created by the community, which often include tools for specific services like Gmail.\n",
    "4.  `Utility Tools`: Special tools that help handle large amounts of data from other tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d47a40",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c70d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index llama-index-vector-stores-chroma llama-index-llms-huggingface-api llama-index-embeddings-huggingface llama-index-tools-google -Uqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea04b8",
   "metadata": {},
   "source": [
    "## Create a Function Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a8c583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather for Hong Kong...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolOutput(content='The weather in Hong Kong is cloudy.', tool_name='my_weather_tool', raw_input={'args': ('Hong Kong',), 'kwargs': {}}, raw_output='The weather in Hong Kong is cloudy.', is_error=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def get_weather(location: str) -> str: \n",
    "    \"\"\"Useful for getting weather information for a given location.\"\"\"\n",
    "    print(f\"Getting weather for {location}...\")\n",
    "    return f\"The weather in {location} is cloudy.\"\n",
    "\n",
    "tool = FunctionTool.from_defaults(\n",
    "    get_weather, \n",
    "    name=\"my_weather_tool\", \n",
    "    description=\"Useful for getting weather information for a given location.\"\n",
    ")\n",
    "\n",
    "tool.call(\"Hong Kong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f6cd527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolOutput(content='Overweight', tool_name='bmi', raw_input={'args': (70, 1.6), 'kwargs': {}}, raw_output='Overweight', is_error=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bmi_calculator(weight: float, height: float):\n",
    "    \"\"\"\n",
    "    This tool calculates the Body Mass Index (BMI) based on weight (kilograms) and height (meters).\n",
    "    It returns a BMI category (Underweight, Normal, Overweight, or Obese).\n",
    "    \"\"\"\n",
    "    bmi = weight / (height ** 2)\n",
    "\n",
    "    if bmi <= 18.5:\n",
    "        return \"Underweight\"\n",
    "    elif bmi <= 25.0:\n",
    "        return \"Normal\"\n",
    "    elif bmi <= 30.0:\n",
    "        return \"Overweight\"\n",
    "    else:\n",
    "        return \"Obese\"\n",
    "    \n",
    "tool = FunctionTool.from_defaults(\n",
    "    bmi_calculator, \n",
    "    name=\"bmi\", \n",
    "    description=\"\"\"This tool calculates the Body Mass Index (BMI) based on weight (kilograms) and height (meters).\n",
    "    It returns a BMI category (Underweight, Normal, Overweight, or Obese).\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "tool.call(70, 1.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b8183",
   "metadata": {},
   "source": [
    "## Create a QueryEngineTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5547adf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolOutput(content=\"<think>\\nOkay, the user is asking about how to respond to research on AI's impact on the future of work and society, but they provided context about a social justice educator focused on diversity, equity, and inclusion. Let me check the context again.\\n\\nThe context mentions someone working with families and communities to promote empathy and understanding of intersectional identity and oppression. So the answer should tie AI's impact to these areas. Maybe discuss how AI could affect marginalized groups, access to opportunities, or equity in the workforce. Need to ensure it's from the perspective of someone advocating for DEI. Avoid mentioning the context directly. Focus on how AI might exacerbate existing inequalities or create new challenges in equity and inclusion. Also, consider the societal implications like bias in AI systems and the need for inclusive policies. Make sure the answer aligns with the educator's role in addressing these issues through education and community work. Keep it concise and relevant to the query without referencing the context explicitly.\\n</think>\\n\\nThe focus on diversity, equity, and inclusion highlights the need to examine how AI technologies might perpetuate or mitigate systemic inequities in the workforce and society. Research should explore AI's potential to reshape labor markets, particularly for marginalized communities, and its capacity to either amplify existing biases or foster more equitable opportunities through intentional design and policy frameworks. Addressing these dynamics requires centering intersectional perspectives to ensure AI advancements align with principles of justice and accessibility.\", tool_name='some useful name', raw_input={'input': 'Responds about research on the impact of AI on the future of work and society?'}, raw_output=Response(response=\"<think>\\nOkay, the user is asking about how to respond to research on AI's impact on the future of work and society, but they provided context about a social justice educator focused on diversity, equity, and inclusion. Let me check the context again.\\n\\nThe context mentions someone working with families and communities to promote empathy and understanding of intersectional identity and oppression. So the answer should tie AI's impact to these areas. Maybe discuss how AI could affect marginalized groups, access to opportunities, or equity in the workforce. Need to ensure it's from the perspective of someone advocating for DEI. Avoid mentioning the context directly. Focus on how AI might exacerbate existing inequalities or create new challenges in equity and inclusion. Also, consider the societal implications like bias in AI systems and the need for inclusive policies. Make sure the answer aligns with the educator's role in addressing these issues through education and community work. Keep it concise and relevant to the query without referencing the context explicitly.\\n</think>\\n\\nThe focus on diversity, equity, and inclusion highlights the need to examine how AI technologies might perpetuate or mitigate systemic inequities in the workforce and society. Research should explore AI's potential to reshape labor markets, particularly for marginalized communities, and its capacity to either amplify existing biases or foster more equitable opportunities through intentional design and policy frameworks. Addressing these dynamics requires centering intersectional perspectives to ensure AI advancements align with principles of justice and accessibility.\", source_nodes=[NodeWithScore(node=TextNode(id_='c6013a18-c856-4209-934a-0e66feb5568d', embedding=None, metadata={'file_path': '/Users/karen/Documents/02_AIE Journey/02_Study/05_Agents/01_HF Agents Course/data/persona_10.txt', 'file_name': 'persona_10.txt', 'file_type': 'text/plain', 'file_size': 207, 'creation_date': '2025-05-16', 'last_modified_date': '2025-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6eb674fc-ae92-494c-b2e3-c702f53cade7', node_type='4', metadata={'file_path': '/Users/karen/Documents/02_AIE Journey/02_Study/05_Agents/01_HF Agents Course/data/persona_10.txt', 'file_name': 'persona_10.txt', 'file_type': 'text/plain', 'file_size': 207, 'creation_date': '2025-05-16', 'last_modified_date': '2025-05-16'}, hash='e5cfcdfbeba0e72b369b9c8f499d77ccd237dadba54beb3df160599ade1adf72')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='A social justice educator or activist focused on diversity, equity, and inclusion, likely working with families and communities to promote empathy and understanding of intersectional identity and oppression.', mimetype='text/plain', start_char_idx=0, end_char_idx=207, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.39856884881537774), NodeWithScore(node=TextNode(id_='233731c9-9cdb-4e6e-a548-1659e7327557', embedding=None, metadata={'file_path': '/Users/karen/Documents/02_AIE Journey/02_Study/05_Agents/01_HF Agents Course/data/persona_10.txt', 'file_name': 'persona_10.txt', 'file_type': 'text/plain', 'file_size': 207, 'creation_date': '2025-05-16', 'last_modified_date': '2025-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2e578036-21c9-467e-ad5d-6954c496858c', node_type='4', metadata={'file_path': '/Users/karen/Documents/02_AIE Journey/02_Study/05_Agents/01_HF Agents Course/data/persona_10.txt', 'file_name': 'persona_10.txt', 'file_type': 'text/plain', 'file_size': 207, 'creation_date': '2025-05-16', 'last_modified_date': '2025-05-16'}, hash='e5cfcdfbeba0e72b369b9c8f499d77ccd237dadba54beb3df160599ade1adf72')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='A social justice educator or activist focused on diversity, equity, and inclusion, likely working with families and communities to promote empathy and understanding of intersectional identity and oppression.', mimetype='text/plain', start_char_idx=0, end_char_idx=207, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.39856884881537774)], metadata={'c6013a18-c856-4209-934a-0e66feb5568d': {'file_path': '/Users/karen/Documents/02_AIE Journey/02_Study/05_Agents/01_HF Agents Course/data/persona_10.txt', 'file_name': 'persona_10.txt', 'file_type': 'text/plain', 'file_size': 207, 'creation_date': '2025-05-16', 'last_modified_date': '2025-05-16'}, '233731c9-9cdb-4e6e-a548-1659e7327557': {'file_path': '/Users/karen/Documents/02_AIE Journey/02_Study/05_Agents/01_HF Agents Course/data/persona_10.txt', 'file_name': 'persona_10.txt', 'file_type': 'text/plain', 'file_size': 207, 'creation_date': '2025-05-16', 'last_modified_date': '2025-05-16'}}), is_error=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "llm = Ollama(model=\"qwen3:8b\", request_timeout=60.0)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, embed_model=embed_model\n",
    ")\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"persona_db\",\n",
    "    description=\"This tool query persona databse.\",\n",
    ")\n",
    "await tool.acall(\n",
    "    \"Responds about research on the impact of AI on the future of work and society?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b293d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='some useful description', name='some useful name', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd38ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await tool.acall(\n",
    "    \"With the persona of a data analyst, what tool do they frequently use?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c1a76f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolOutput(content=\"<think>\\nOkay, let's see. The user is asking about a data analyst's frequently used tool, but the context provided is about a web developer or student focused on HTML, CSS, and website building. The persona mentioned doesn't relate to data analysis. Since the instructions say to use only the given context and not prior knowledge, I can't assume any tools related to data analysis. The answer should stick to the context, which doesn't mention data analysts. So, there's no information here about data analysts or their tools. I need to make sure not to reference the context directly and avoid mentioning the context in the answer. Maybe the answer is that the context doesn't provide info on data analysts, but the user wants a tool. Wait, but the persona is a web developer, so maybe the tools they use are related to web development like code editors or browsers. But the question specifically mentions a data analyst. Since the context doesn't cover that, I should state that the provided information doesn't address data analysts. But the user wants an answer based on the context. Hmm, maybe the answer is that the context doesn't mention data analysts, so there's no tool to specify. But the user might expect an answer. Wait, the instructions say not to use prior knowledge. So if the context doesn't have info on data analysts, the answer can't be inferred. Therefore, the correct response is that the context doesn't provide information about data analysts or their tools.\\n</think>\\n\\nThe context provided does not mention data analysts or their tools.\", tool_name='some useful name', raw_input={'input': 'With the persona of a data analyst, what tool do they frequently use?'}, raw_output=Response(response=\"<think>\\nOkay, let's see. The user is asking about a data analyst's frequently used tool, but the context provided is about a web developer or student focused on HTML, CSS, and website building. The persona mentioned doesn't relate to data analysis. Since the instructions say to use only the given context and not prior knowledge, I can't assume any tools related to data analysis. The answer should stick to the context, which doesn't mention data analysts. So, there's no information here about data analysts or their tools. I need to make sure not to reference the context directly and avoid mentioning the context in the answer. Maybe the answer is that the context doesn't provide info on data analysts, but the user wants a tool. Wait, but the persona is a web developer, so maybe the tools they use are related to web development like code editors or browsers. But the question specifically mentions a data analyst. Since the context doesn't cover that, I should state that the provided information doesn't address data analysts. But the user wants an answer based on the context. Hmm, maybe the answer is that the context doesn't mention data analysts, so there's no tool to specify. But the user might expect an answer. Wait, the instructions say not to use prior knowledge. So if the context doesn't have info on data analysts, the answer can't be inferred. Therefore, the correct response is that the context doesn't provide information about data analysts or their tools.\\n</think>\\n\\nThe context provided does not mention data analysts or their tools.\", source_nodes=[NodeWithScore(node=TextNode(id_='b643b3ed-5a30-4bed-ab29-7cb422ee0af9', embedding=None, metadata={'file_path': '/Users/karen/Documents/02_AIE Journey/02_Study/05_Agents/01_HF Agents Course/data/persona_1001.txt', 'file_name': 'persona_1001.txt', 'file_type': 'text/plain', 'file_size': 157, 'creation_date': '2025-05-16', 'last_modified_date': '2025-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='efc3cd3f-f49e-4320-8001-a6604605f8b0', node_type='4', metadata={'file_path': '/Users/karen/Documents/02_AIE Journey/02_Study/05_Agents/01_HF Agents Course/data/persona_1001.txt', 'file_name': 'persona_1001.txt', 'file_type': 'text/plain', 'file_size': 157, 'creation_date': '2025-05-16', 'last_modified_date': '2025-05-16'}, hash='fa1d4ba5d5b216f320e6ec1aad52505594e8a8e85f567e377e2a10d003c6d63d')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='A web developer or a web development student, likely in the early stages of their learning or career, with a strong focus on HTML, CSS, and website building.', mimetype='text/plain', start_char_idx=0, end_char_idx=157, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.46592620182714595), NodeWithScore(node=TextNode(id_='7d0053e0-a7bf-40ba-a7f0-d1951d510256', embedding=None, metadata={'file_path': '/Users/karen/Documents/02_AIE Journey/02_Study/05_Agents/01_HF Agents Course/data/persona_1001.txt', 'file_name': 'persona_1001.txt', 'file_type': 'text/plain', 'file_size': 157, 'creation_date': '2025-05-16', 'last_modified_date': '2025-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ad9f8220-b402-461e-a1e3-c7b309e725fb', node_type='4', metadata={'file_path': '/Users/karen/Documents/02_AIE Journey/02_Study/05_Agents/01_HF Agents Course/data/persona_1001.txt', 'file_name': 'persona_1001.txt', 'file_type': 'text/plain', 'file_size': 157, 'creation_date': '2025-05-16', 'last_modified_date': '2025-05-16'}, hash='fa1d4ba5d5b216f320e6ec1aad52505594e8a8e85f567e377e2a10d003c6d63d')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='A web developer or a web development student, likely in the early stages of their learning or career, with a strong focus on HTML, CSS, and website building.', mimetype='text/plain', start_char_idx=0, end_char_idx=157, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.46592620182714595)], metadata={'b643b3ed-5a30-4bed-ab29-7cb422ee0af9': {'file_path': '/Users/karen/Documents/02_AIE Journey/02_Study/05_Agents/01_HF Agents Course/data/persona_1001.txt', 'file_name': 'persona_1001.txt', 'file_type': 'text/plain', 'file_size': 157, 'creation_date': '2025-05-16', 'last_modified_date': '2025-05-16'}, '7d0053e0-a7bf-40ba-a7f0-d1951d510256': {'file_path': '/Users/karen/Documents/02_AIE Journey/02_Study/05_Agents/01_HF Agents Course/data/persona_1001.txt', 'file_name': 'persona_1001.txt', 'file_type': 'text/plain', 'file_size': 157, 'creation_date': '2025-05-16', 'last_modified_date': '2025-05-16'}}), is_error=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c35988d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The user is asking about a data analyst's frequently used tool, but the context provided is about a web developer or student focused on HTML, CSS, and website building. The persona mentioned doesn't relate to data analysis. Since the instructions say to use only the given context and not prior knowledge, I can't assume any tools related to data analysis. The answer should stick to the context, which doesn't mention data analysts. So, there's no information here about data analysts or their tools. I need to make sure not to reference the context directly and avoid mentioning the context in the answer. Maybe the answer is that the context doesn't provide info on data analysts, but the user wants a tool. Wait, but the persona is a web developer, so maybe the tools they use are related to web development like code editors or browsers. But the question specifically mentions a data analyst. Since the context doesn't cover that, I should state that the provided information doesn't address data analysts. But the user wants an answer based on the context. Hmm, maybe the answer is that the context doesn't mention data analysts, so there's no tool to specify. But the user might expect an answer. Wait, the instructions say not to use prior knowledge. So if the context doesn't have info on data analysts, the answer can't be inferred. Therefore, the correct response is that the context doesn't provide information about data analysts or their tools.\n",
      "</think>\n",
      "\n",
      "The context provided does not mention data analysts or their tools.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97210dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, let's see. The user is asking about a data analyst's frequently used tool, but the context provided is about a web developer or student focused on HTML, CSS, and website building. The persona mentioned doesn't relate to data analysis. Since the instructions say to use only the given context and not prior knowledge, I can't assume any tools related to data analysis. The answer should stick to the context, which doesn't mention data analysts. So, there's no information here about data analysts or their tools. I need to make sure not to reference the context directly and avoid mentioning the context in the answer. Maybe the answer is that the context doesn't provide info on data analysts, but the user wants a tool. Wait, but the persona is a web developer, so maybe the tools they use are related to web development like code editors or browsers. But the question specifically mentions a data analyst. Since the context doesn't cover that, I should state that the provided information doesn't address data analysts. But the user wants an answer based on the context. Hmm, maybe the answer is that the context doesn't mention data analysts, so there's no tool to specify. But the user might expect an answer. Wait, the instructions say not to use prior knowledge. So if the context doesn't have info on data analysts, the answer can't be inferred. Therefore, the correct response is that the context doesn't provide information about data analysts or their tools.\\n</think>\\n\\nThe context provided does not mention data analysts or their tools.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24e725ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.is_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac3f2d",
   "metadata": {},
   "source": [
    "## Create Toolspecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb19c4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llama_index.core.tools.function_tool.FunctionTool at 0x350bfa810>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x3507bfe90>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x350bfbb30>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x350bfa510>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x350bf8b30>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x350bf9d90>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.tools.google import GmailToolSpec\n",
    "\n",
    "tool_spec = GmailToolSpec()\n",
    "tool_spec_list = tool_spec.to_tool_list()\n",
    "tool_spec_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6eb25",
   "metadata": {},
   "source": [
    "To get a more detailed view of the tools, we can take a look at the `metadata` of each tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0441b77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('load_data',\n",
       "  \"load_data() -> List[llama_index.core.schema.Document]\\nLoad emails from the user's account.\"),\n",
       " ('search_messages',\n",
       "  \"search_messages(query: str, max_results: Optional[int] = None)\\nSearches email messages given a query string and the maximum number\\n        of results requested by the user\\n           Returns: List of relevant message objects up to the maximum number of results.\\n\\n        Args:\\n            query[str]: The user's query\\n            max_results (Optional[int]): The maximum number of search results\\n            to return.\\n        \"),\n",
       " ('create_draft',\n",
       "  \"create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -> str\\nCreate and insert a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            to (Optional[str]): The email addresses to send the message to\\n            subject (Optional[str]): The subject for the event\\n            message (Optional[str]): The message for the event\\n        \"),\n",
       " ('update_draft',\n",
       "  \"update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -> str\\nUpdate a draft email.\\n           Print the returned draft's message and id.\\n           This function is required to be passed a draft_id that is obtained when creating messages\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            to (Optional[str]): The email addresses to send the message to\\n            subject (Optional[str]): The subject for the event\\n            message (Optional[str]): The message for the event\\n            draft_id (str): the id of the draft to be updated\\n        \"),\n",
       " ('get_draft',\n",
       "  \"get_draft(draft_id: str = None) -> str\\nGet a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            draft_id (str): the id of the draft to be updated\\n        \"),\n",
       " ('send_draft',\n",
       "  \"send_draft(draft_id: str = None) -> str\\nSends a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            draft_id (str): the id of the draft to be updated\\n        \")]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(tool.metadata.name, tool.metadata.description) for tool in tool_spec_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b148984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description=\"load_data() -> List[llama_index.core.schema.Document]\\nLoad emails from the user's account.\", name='load_data', fn_schema=<class 'llama_index.core.tools.utils.load_data'>, return_direct=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_spec_list[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a4e3f",
   "metadata": {},
   "source": [
    "## Model Context Protocol (MCP) in LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d923405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-tools-mcp -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7eb4b724",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m mcp_tool = McpToolSpec(client=mcp_client)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Get agent\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m agent = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mget_agent\u001b[49m(mcp_tool)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Create agent context\u001b[39;00m\n\u001b[32m     10\u001b[39m agnet_context = Context(agent)\n",
      "\u001b[31mNameError\u001b[39m: name 'get_agent' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.tools.mcp import BasicMCPClient, McpToolSpec\n",
    "\n",
    "mcp_client = BasicMCPClient(\"http://127.0.0.1:8000/sse\")\n",
    "mcp_tool = McpToolSpec(client=mcp_client)\n",
    "\n",
    "# Get agent\n",
    "agent = await get_agent(mcp_tool)\n",
    "\n",
    "# Create agent context\n",
    "agnet_context = Context(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2ba30",
   "metadata": {},
   "source": [
    "## Utility Tools\n",
    "You can find toolspecs and utility tools on theÂ [LlamaHub](https://llamahub.ai/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef97423",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
